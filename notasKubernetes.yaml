kubectl cluster-info
kubectl get nodes
# shows currently used nodes
kubectl run kubernetes-bootcamp --image=gcr.io/google-samples/kubernetes-bootcamp:v1 --port=8080
# deploys this image using the 8080 port
kubectl get deployments
# shows the current deployments
kubectl proxy
# provides a connection between the online host and the cluster
export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
echo Name of the Pod: $POD_NAME
# Assigns a name to the pod to an environment variable
curl http://localhost:8001/api/v1/namespaces/default/pods/$POD_NAME/proxy/
# provides information about the requested pod
kubectl get - list resources
kubectl describe - show detailed information about a resource
kubectl logs - print the logs from a container in a pod
kubectl exec - execute a command on a container in a pod
kubectl exec $POD_NAME bash
# opens a terminal in the specified pod. Same behaviour as a container
kubectl expose deployment/kubernetes-bootcamp --type="NodePort" --port 8080
# Creates a service and exposes it to external traffic
export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
echo NODE_PORT=$NODE_PORT
# Create an environment variable called NODE_PORT that has the value of the Node port assigned
kubectl get pods -l run=kubernetes-bootcamp
# Queries the list of pods to get the pods used by this label
kubectl get services -l run=kubernetes-bootcamp
# Queries the list of services to get the pods used by this label
export POD_NAME=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{"\n"}}{{end}}')
echo Name of the Pod: $POD_NAME
# Get pod name
kubectl label pod $POD_NAME app=v1
# Label the pod
kubectl get pods -l app=v1
# Can use the pod alias instead
kubectl delete service -l run=kubernetes-bootcamp
# Deletes the kubernetes-bootcamp service
# The app will not be reachable from outside the cluster
kubectl scale deployments/kubernetes-bootcamp --replicas=4
# Launches 4 replicas of the desired pod
kubectl get pods -o wide
# Lists all pods
kubectl describe deployments/kubernetes-bootcamp
# Should list all replicas
export NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')
echo NODE_PORT=$NODE_PORT
# Creates an env variable for the NODE_PORT
curl $(minikube ip):$NODE_PORT
# A different POD is hit with every request, this proves that the load balancing is working correctly
kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2
# Notifies running pods to use a different version of the app. This command automatically rolls out and update on running pods
kubectl rollout status deployments/kubernetes-bootcamp
# The update can be confirmed also by running a rollout status command
kubectl set image deployments/kubernetes-bootcamp kubernetes-bootcamp=gcr.io/google-samples/kubernetes-bootcamp:v10
# Rolls out a new update with an unexistant image
kubectl rollout undo deployments/kubernetes-bootcamp
# Reverts the deployment to the previous known state of the image

Example of a POD in a yaml file:

apiVersion: v1
kind: Pod
metadata:
    name: my-app
spec:
    containers:
    -   name: my-app
        image: my-app
    -   name: nginx-ssl
        image: nginx
        ports:
        -   containerPort: 80
        -   containerPort: 443

#############################################
# This file has to be uploaded to the master, then the master will create pods on the nodes of the cluster it's managing

Deployment file:

kind: Deployment #deployment resource
apiVersion: v1.1
metadata:
    name: frontend  #deployment name
spec:
    replicas: 4 #replicas
    selector:   #pod selector
        role: web   #role web
    template:
        metadata:
            name: web
            labels: #pod label
                role: web #role web
        spec:
            containers: #pods
            - name: my-app
              image: my-app
            - name: nginx-ssl
              image: nginx
              ports:
              - containerPort: 80
              - containerPort: 443

###############################################
# A service assigns a fixed IP to your pod replicas and allows other pods or services to communicate with them

Example of a Service file:

kind: Service #resource
apiVersion: v1
metadata:
    name: web-frontend
spec:
    ports:
    - name: http
      port: 80
      targetPort: 80
      protocol: TCP
    selector: #pod selector
        role: web
    type: LoadBalancer

###############################################

kubectl create <volume> 
# Creates a volume
kubectl create -f pod.yml
# Creates a volume according to the file specs
# The volume is attached to the pod and made available to containers before they are brought online
# Some volumes share the lifecycle of their pod
minikube addons enable dashboard
# Make the Kubernetes Dashboard available by deploying the following YAML definition. This should only be used on Katacoda.
kubectl apply -f /opt/kubernetes-dashboard.yaml
# Shows a dashboard
# Should only use it in Katacoda.
kubeadm init --token=102952.1a7dd4cc8d1f4cc5 --kubernetes-version $(kubeadm version -o short)
# The command above will initialise the cluster with a known token to simplify the following steps.
# To manage the Kubernetes cluster, the client configuration and certificates are required. This configuration is created when kubeadm initialises the cluster. The command copies the configuration to the users home directory and sets the environment variable for use with the CLI.
sudo cp /etc/kubernetes/admin.conf $HOME/
sudo chown $(id -u):$(id -g) $HOME/admin.conf
export KUBECONFIG=$HOME/admin.conf
########
kubectl apply -f /opt/weave-kube
# Weave will now deploy as a series of Pods on the cluster. The status of this can be viewed using the command 
kubectl get pod -n kube-system
#####
kubeadm token list
# Lists tokens
kubeadm join --discovery-token-unsafe-skip-ca-verification --token=102952.1a7dd4cc8d1f4cc5 172.17.0.30:6443
# Running this command on a node will make a node join a cluster skipping verification
# In production, the token is provided by running: kubeadm init
kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep admin-user | awk '{print $1}')
# Gets login token
kubectl expose deployment http --external-ip="172.17.0.47" --port=8000 --target-port=80
# This command exposes the CONTAINER port 80 on the HOST 8000 binding it to the EXTERNAL IP 172.17.0.47

deployment.yaml:

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: webapp1
    spec:
      containers:
      - name: webapp1
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80

#################################################
# This file deploys an app called webapp1 using the docker image katacoda/docker-http-server:latest on the port 80
kubectl create -f deployment.yaml
# This command deploys it to the cluster
kubectl get deployment
# Verify it's created
kubectl describe deployment webapp1
# Get details about the deployment

service.yaml:

apiVersion: v1
kind: Service
metadata:
  name: webapp1-svc
  labels:
    app: webapp1
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30080
  selector:
    app: webapp1

######################################
# This service selects all applications with the label webapp1
# As multiple replicas, or instances, are deployed, they will be automatically load balanced based on this common label. The Service makes the application available via a NodePort.
kubectl create -f service.yaml
# This command deploys the service
kubectl get svc
# Verify it's running
kubectl describe svc webapp1-svc
# Get more information about the service
# deployment.yaml is updated
kubectl apply -f deployment.yaml
# kubectl apply deploys the new file, creating the required replicas

clusterip.yaml:

apiVersion: v1
kind: Service
metadata:
  name: webapp1-clusterip-svc
  labels:
    app: webapp1-clusterip
spec:
  ports:
  - port: 80
  selector:
    app: webapp1-clusterip
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-clusterip-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-clusterip
    spec:
      containers:
      - name: webapp1-clusterip-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---

##################################
# This will deploy a web app with two replicas to showcase load balancing along with a service.
# It will also deploy a service
export CLUSTER_IP=$(kubectl get services/webapp1-clusterip-svc -o go-template='{{(index .spec.clusterIP)}}')
echo CLUSTER_IP=$CLUSTER_IP
curl $CLUSTER_IP:80
# Exports the cluster ip to an environment variable

clusterip-target.yaml:

apiVersion: v1
kind: Service
metadata:
  name: webapp1-clusterip-targetport-svc
  labels:
    app: webapp1-clusterip-targetport
spec:
  ports:
  - port: 8080
    targetPort: 80
  selector:
    app: webapp1-clusterip-targetport
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-clusterip-targetport-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-clusterip-targetport
    spec:
      containers:
      - name: webapp1-clusterip-targetport-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---

##########################################
# Changes the targetPort to 8080

nodeport.yaml:

apiVersion: v1
kind: Service
metadata:
  name: webapp1-nodeport-svc
  labels:
    app: webapp1-nodeport
spec:
  type: NodePort
  ports:
  - port: 80
    nodePort: 30080
  selector:
    app: webapp1-nodeport
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-nodeport-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-nodeport
    spec:
      containers:
      - name: webapp1-nodeport-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
####################################################
# TargetPort and ClusterIP make it available inside the cluster, NodePort exposes the service on each node's IP via the defined static port
# No matter what port is accessed, the service will be reachable based on the port number defined

externalip.yaml:

apiVersion: v1
kind: Service
metadata:
  name: webapp1-externalip-svc
  labels:
    app: webapp1-externalip
spec:
  ports:
  - port: 80
  externalIPs:
  - HOSTIP
  selector:
    app: webapp1-externalip
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-externalip-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-externalip
    spec:
      containers:
      - name: webapp1-externalip-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
############################################
sed -i 's/HOSTIP/172.17.0.13/g' externalip.yaml
# Updates the definition to the current cluster's IP address
kubectl apply -f externalip.yaml
# Applies the update described in the yaml file

cloudprovider.yaml:

apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: kube-keepalived-vip
  namespace: kube-system
spec:
  template:
    metadata:
      labels:
        name: kube-keepalived-vip
    spec:
      hostNetwork: true
      containers:
        - image: gcr.io/google_containers/kube-keepalived-vip:0.9
          name: kube-keepalived-vip
          imagePullPolicy: Always
          securityContext:
            privileged: true
          volumeMounts:
            - mountPath: /lib/modules
              name: modules
              readOnly: true
            - mountPath: /dev
              name: dev
          # use downward API
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          # to use unicast
          args:
          - --services-configmap=kube-system/vip-configmap
          # unicast uses the ip of the nodes instead of multicast
          # this is useful if running in cloud providers (like AWS)
          #- --use-unicast=true
      volumes:
        - name: modules
          hostPath:
            path: /lib/modules
        - name: dev
          hostPath:
            path: /dev
      nodeSelector:
        # type: worker # adjust this to match your worker nodes
---
## We also create an empty ConfigMap to hold our config
apiVersion: v1
kind: ConfigMap
metadata:
  name: vip-configmap
  namespace: kube-system
data:
---
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  labels:
    app: keepalived-cloud-provider
  name: keepalived-cloud-provider
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: keepalived-cloud-provider
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/critical-pod: ""
        scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
      labels:
        app: keepalived-cloud-provider
    spec:
      containers:
      - name: keepalived-cloud-provider
        image: quay.io/munnerz/keepalived-cloud-provider:0.0.1
        imagePullPolicy: IfNotPresent
        env:
        - name: KEEPALIVED_NAMESPACE
          value: kube-system
        - name: KEEPALIVED_CONFIG_MAP
          value: vip-configmap
        - name: KEEPALIVED_SERVICE_CIDR
          value: 10.10.0.0/26 # pick a CIDR that is explicitly reserved for keepalived
        volumeMounts:
        - name: certs
          mountPath: /etc/ssl/certs
        resources:
          requests:
            cpu: 200m
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10252
            host: 127.0.0.1
          initialDelaySeconds: 15
          timeoutSeconds: 15
          failureThreshold: 8
      volumes:
      - name: certs
        hostPath:
          path: /etc/ssl/certs


###################################################
# This file imitates the load balancing done by a real cloud provideer.

loadbalancer.yaml:

apiVersion: v1
kind: Service
metadata:
  name: webapp1-loadbalancer-svc
  labels:
    app: webapp1-loadbalancer
spec:
  type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: webapp1-loadbalancer
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: webapp1-loadbalancer-deployment
spec:
  replicas: 2
  template:
    metadata:
      labels:
        app: webapp1-loadbalancer
    spec:
      containers:
      - name: webapp1-loadbalancer-pod
        image: katacoda/docker-http-server:latest
        ports:
        - containerPort: 80
---
######################################
# When a service requests a Load Balancer, the provider will allocate one from the 10.10.0.0/26 range defined in the configuration.
kubectl apply -f loadbalancer.yaml
# Applies changes
export LoadBalancerIP=$(kubectl get services/webapp1-loadbalancer-svc -o go-template='{{(index .status.loadBalancer.ingress 0).ip}}')
echo LoadBalancerIP=$LoadBalancerIP
curl $LoadBalancerIP
# The service can now be accessed from the given IP


